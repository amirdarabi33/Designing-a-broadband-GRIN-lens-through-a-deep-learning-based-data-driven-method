{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adarabi3\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\backend.py:450: UserWarning: `tf.keras.backend.set_learning_phase` is deprecated and will be removed after 2020-10-11. To update it, simply pass a True/False value to the `training` argument of the `__call__` method of your layer or model.\n",
      "  warnings.warn('`tf.keras.backend.set_learning_phase` is deprecated and '\n"
     ]
    }
   ],
   "source": [
    "from keras import layers\n",
    "from keras import regularizers\n",
    "from keras.layers import Input, Add, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D, AveragePooling2D, MaxPooling2D, GlobalMaxPooling2D\n",
    "from keras.models import Model, load_model\n",
    "from keras.preprocessing import image\n",
    "from keras.utils import layer_utils\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.applications.imagenet_utils import preprocess_input\n",
    "import pydot\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.initializers import glorot_uniform\n",
    "from matplotlib.pyplot import imshow\n",
    "import pandas as pd\n",
    "import os\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import keras.backend as K\n",
    "K.set_image_data_format('channels_last')\n",
    "K.set_learning_phase(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data=[]\n",
    "datadir=\"C:/Users/adarabi3/OneDrive - Georgia Institute of Technology/Documents/MATLAB/Lens_data/Image_output/\"\n",
    "path = os.path.join(datadir)\n",
    "image_list = os.listdir(path)\n",
    "for img in image_list:\n",
    "    image_array=cv2.imread(os.path.join(path,img),cv2.IMREAD_GRAYSCALE)\n",
    "    image_array2=cv2.resize(image_array,(100,100))\n",
    "    # plt.imshow(image_array,cmap=\"gray\")\n",
    "    # plt.show()\n",
    "    # break\n",
    "    training_data.append([image_array2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21000, 11)\n",
      "(21000, 100, 100, 1)\n"
     ]
    }
   ],
   "source": [
    "X=[]\n",
    "for features in training_data:\n",
    "    X.append(features)    \n",
    "X=np.array(X).reshape(-1,100,100,1)\n",
    "Y_train = pd.read_csv(\"Data_param.csv\")\n",
    "Y= Y_train.to_numpy().reshape(-1,11)\n",
    "print(Y.shape)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200, 100, 100, 1)\n",
      "(1000, 100, 100, 1)\n",
      "(200, 11)\n",
      "(1000, 11)\n"
     ]
    }
   ],
   "source": [
    "x_train=X[0:200]#20000]\n",
    "x_test=X[20000:21001]\n",
    "y_train=Y[0:200]#20000]\n",
    "y_test=Y[20000:21001]\n",
    "x_train=x_train/255\n",
    "x_test=x_test/255\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identity_block(X, f, filters, stage, block):\n",
    "    \"\"\"\n",
    "    X -- input tensor of shape (m, n_H_prev, n_W_prev, n_C_prev)\n",
    "    f -- integer, specifying the shape of the middle CONV's window for the main path\n",
    "    filters -- python list of integers, defining the number of filters in the CONV layers of the main path\n",
    "    stage -- integer, used to name the layers, depending on their position in the network\n",
    "    block -- string/character, used to name the layers, depending on their position in the network\n",
    "    \n",
    "    Returns:\n",
    "    X -- output of the identity block, tensor of shape (n_H, n_W, n_C)\n",
    "    \"\"\"\n",
    "    \n",
    "    # defining name basis\n",
    "    conv_name_base = 'res' + str(stage) + block + '_branch'\n",
    "    bn_name_base = 'bn' + str(stage) + block + '_branch'\n",
    "    \n",
    "    # Retrieve Filters\n",
    "    F1, F2, F3 = filters\n",
    "    \n",
    "    X_shortcut = X\n",
    "    \n",
    "    # First component of main path\n",
    "    X = Conv2D(filters = F1, kernel_size = (1, 1), strides = (1,1), padding = 'valid', name = conv_name_base + '2a', kernel_initializer = glorot_uniform(seed=0))(X)\n",
    "    X = BatchNormalization(axis = 3, name = bn_name_base + '2a')(X)\n",
    "    X = Activation('relu')(X)\n",
    "    \n",
    "    X = Conv2D(filters = F2, kernel_size = (f, f), strides = (1,1), padding = 'same', name = conv_name_base + '2b', kernel_initializer = glorot_uniform(seed=0))(X)\n",
    "    X = BatchNormalization(axis = 3, name = bn_name_base + '2b')(X)\n",
    "    X = Activation('relu')(X)\n",
    "\n",
    "    # Third component of main path (≈2 lines)\n",
    "    X = Conv2D(filters = F3, kernel_size = (1, 1), strides = (1,1), padding = 'valid', name = conv_name_base + '2c', kernel_initializer = glorot_uniform(seed=0))(X)\n",
    "    X = BatchNormalization(axis = 3, name = bn_name_base + '2c')(X)\n",
    "\n",
    "    # Final step: Add shortcut value to main path, and pass it through a RELU activation (≈2 lines)\n",
    "    X = Add()([X, X_shortcut])\n",
    "    X = Activation('relu')(X)\n",
    "    \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convolutional_block(X, f, filters, stage, block, s = 2):\n",
    "    \"\"\"\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input tensor of shape (m, n_H_prev, n_W_prev, n_C_prev)\n",
    "    f -- integer, specifying the shape of the middle CONV's window for the main path\n",
    "    filters -- python list of integers, defining the number of filters in the CONV layers of the main path\n",
    "    stage -- integer, used to name the layers, depending on their position in the network\n",
    "    block -- string/character, used to name the layers, depending on their position in the network\n",
    "    s -- Integer, specifying the stride to be used\n",
    "    \n",
    "    Returns:\n",
    "    X -- output of the convolutional block, tensor of shape (n_H, n_W, n_C)\n",
    "    \"\"\"\n",
    "    \n",
    "    # defining name basis\n",
    "    conv_name_base = 'res' + str(stage) + block + '_branch'\n",
    "    bn_name_base = 'bn' + str(stage) + block + '_branch'\n",
    "    \n",
    "    # Retrieve Filters\n",
    "    F1, F2, F3 = filters\n",
    "    \n",
    "    # Save the input value\n",
    "    X_shortcut = X\n",
    "\n",
    "\n",
    "    ##### MAIN PATH #####\n",
    "    # First component of main path \n",
    "    X = Conv2D(F1, (1, 1), strides = (s,s), padding = 'valid', name = conv_name_base + '2a', kernel_initializer = glorot_uniform(seed=0))(X)\n",
    "    X = BatchNormalization(axis = 3, name = bn_name_base + '2a')(X)\n",
    "    X = Activation('relu')(X)\n",
    "    # Second component of main path\n",
    "    X = Conv2D(F2,  kernel_size = (f, f), strides = (1,1), padding = 'same', name = conv_name_base + '2b', kernel_initializer = glorot_uniform(seed=0))(X)\n",
    "    X = BatchNormalization(axis = 3, name = bn_name_base + '2b')(X)\n",
    "    X = Activation('relu')(X)\n",
    "\n",
    "    # Third component of main path \n",
    "    X = Conv2D(F3, kernel_size = (1, 1), strides = (1,1), padding = 'valid', name = conv_name_base + '2c', kernel_initializer = glorot_uniform(seed=0))(X)\n",
    "    X = BatchNormalization(axis = 3, name = bn_name_base + '2c')(X)\n",
    "\n",
    "    ##### SHORTCUT PATH #### \n",
    "    X_shortcut = Conv2D(F3, (1, 1), strides = (s,s), padding = 'valid', name = conv_name_base + '1', kernel_initializer = glorot_uniform(seed=0))(X_shortcut)\n",
    "    X_shortcut = BatchNormalization(axis = 3, name = bn_name_base + '1')(X_shortcut)\n",
    "\n",
    "    # Final step: Add shortcut value to main path, and pass it through a RELU activation (≈2 lines)\n",
    "    X = Add()([X, X_shortcut])\n",
    "    X = Activation('relu')(X) \n",
    "    \n",
    "    \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ResNet50(input_shape = (400, 400, 1), classes = 10):\n",
    "    \"\"\"\n",
    "    Implementation of the popular ResNet50 the following architecture:\n",
    "    CONV2D -> BATCHNORM -> RELU -> MAXPOOL -> CONVBLOCK -> IDBLOCK*2 -> CONVBLOCK -> IDBLOCK*2-> AVGPOOL -> TOPLAYER\n",
    "\n",
    "    Arguments:\n",
    "    input_shape -- shape of the images of the dataset\n",
    "    classes -- integer, number of classes\n",
    "\n",
    "    Returns:\n",
    "    model -- a Model() instance in Keras\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define the input as a tensor with shape input_shape\n",
    "    X_input = Input(input_shape)\n",
    "\n",
    "    \n",
    "    # Zero-Padding\n",
    "    X = ZeroPadding2D((3, 3))(X_input)\n",
    "    \n",
    "    # Stage 1\n",
    "    X = Conv2D(64, (2, 2), strides = (2, 2), name = 'conv1', kernel_initializer = glorot_uniform(seed=0))(X)\n",
    "    # picture becomes 100*100*16\n",
    "    X = BatchNormalization(axis = 3, name = 'bn_conv1')(X)\n",
    "    X = Activation('relu')(X)\n",
    "    X = MaxPooling2D((2, 2), strides=(2, 2))(X)\n",
    "    # piture bcomes 25*25*64\n",
    "\n",
    "    # Stage 2\n",
    "    X = convolutional_block(X, f = 3, filters = [16, 16, 32], stage = 2, block='a', s = 1)\n",
    "    # X = identity_block(X, 3, [8, 8, 16], stage=2, block='b')\n",
    "    # picture becomes 13*13*16\n",
    "\n",
    "    # Stage 3\n",
    "    X = convolutional_block(X, f=3, filters=[32, 32, 64], stage=3, block='a', s=2)\n",
    "    # X = identity_block(X, 3, [16, 16, 32], stage=3, block='b')\n",
    "    # picture becomes 7*7*32\n",
    "\n",
    "    # Stage 4\n",
    "    X = convolutional_block(X, f=3, filters=[64, 64, 128], stage=4, block='a', s=2)\n",
    "    # X = identity_block(X, 3, [32, 32, 64], stage=4, block='b')\n",
    "    # picture becomes 4*4*64\n",
    "\n",
    "    # Stage 5\n",
    "    X = convolutional_block(X, f=3, filters=[128, 128, 256], stage=5, block='a', s=2)\n",
    "    # X = identity_block(X, 3, [16, 16, 32], stage=5, block='b')\n",
    "    # picture becomes 2*2*128\n",
    "\n",
    "    # Stage 6\n",
    "    X = convolutional_block(X, f=3, filters=[256, 256, 512], stage=6, block='a', s=2)\n",
    "    # X = identity_block(X, 3, [128, 128, 256], stage=6, block='b')\n",
    "    # X = identity_block(X, 3, [128, 128, 256], stage=6, block='c')\n",
    "    # picture becomes 1*1*256\n",
    "\n",
    "    # # Stage 7\n",
    "    # X = convolutional_block(X, f=3, filters=[16, 16, 16], stage=7, block='a', s=2)\n",
    "    # X = identity_block(X, 3, [256, 256, 512], stage=7, block='b')\n",
    "    # # picture becomes 1*1*512\n",
    "\n",
    "    # Average Pooling\n",
    "    X = AveragePooling2D((2, 2), name='avg_pool')(X)\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    # output layer\n",
    "    X = Flatten()(X)\n",
    "    X = Dense(classes, activation='sigmoid', name='fc' + str(classes), kernel_initializer = glorot_uniform(seed=0))(X)#,kernel_regularizer=tf.keras.regularizers.l1(0.1))(X)\n",
    "    \n",
    "    \n",
    "    # Create model\n",
    "    model = Model(inputs = X_input, outputs = X, name='ResNet50')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ResNet50(input_shape = (100, 100, 1), classes = 11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.compile(optimizer='ADAM', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.compile(tf.keras.optimizers.Adam(learning_rate=0.001), loss=tf.keras.losses.MeanSquaredLogarithmicError(), metrics=['accuracy'])\n",
    "# model.compile(tf.keras.optimizers.Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"ResNet50\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 100, 100, 1  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " zero_padding2d (ZeroPadding2D)  (None, 106, 106, 1)  0          ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " conv1 (Conv2D)                 (None, 53, 53, 64)   320         ['zero_padding2d[0][0]']         \n",
      "                                                                                                  \n",
      " bn_conv1 (BatchNormalization)  (None, 53, 53, 64)   256         ['conv1[0][0]']                  \n",
      "                                                                                                  \n",
      " activation (Activation)        (None, 53, 53, 64)   0           ['bn_conv1[0][0]']               \n",
      "                                                                                                  \n",
      " max_pooling2d (MaxPooling2D)   (None, 26, 26, 64)   0           ['activation[0][0]']             \n",
      "                                                                                                  \n",
      " res2a_branch2a (Conv2D)        (None, 26, 26, 16)   1040        ['max_pooling2d[0][0]']          \n",
      "                                                                                                  \n",
      " bn2a_branch2a (BatchNormalizat  (None, 26, 26, 16)  64          ['res2a_branch2a[0][0]']         \n",
      " ion)                                                                                             \n",
      "                                                                                                  \n",
      " activation_1 (Activation)      (None, 26, 26, 16)   0           ['bn2a_branch2a[0][0]']          \n",
      "                                                                                                  \n",
      " res2a_branch2b (Conv2D)        (None, 26, 26, 16)   2320        ['activation_1[0][0]']           \n",
      "                                                                                                  \n",
      " bn2a_branch2b (BatchNormalizat  (None, 26, 26, 16)  64          ['res2a_branch2b[0][0]']         \n",
      " ion)                                                                                             \n",
      "                                                                                                  \n",
      " activation_2 (Activation)      (None, 26, 26, 16)   0           ['bn2a_branch2b[0][0]']          \n",
      "                                                                                                  \n",
      " res2a_branch2c (Conv2D)        (None, 26, 26, 32)   544         ['activation_2[0][0]']           \n",
      "                                                                                                  \n",
      " res2a_branch1 (Conv2D)         (None, 26, 26, 32)   2080        ['max_pooling2d[0][0]']          \n",
      "                                                                                                  \n",
      " bn2a_branch2c (BatchNormalizat  (None, 26, 26, 32)  128         ['res2a_branch2c[0][0]']         \n",
      " ion)                                                                                             \n",
      "                                                                                                  \n",
      " bn2a_branch1 (BatchNormalizati  (None, 26, 26, 32)  128         ['res2a_branch1[0][0]']          \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " add (Add)                      (None, 26, 26, 32)   0           ['bn2a_branch2c[0][0]',          \n",
      "                                                                  'bn2a_branch1[0][0]']           \n",
      "                                                                                                  \n",
      " activation_3 (Activation)      (None, 26, 26, 32)   0           ['add[0][0]']                    \n",
      "                                                                                                  \n",
      " res3a_branch2a (Conv2D)        (None, 13, 13, 32)   1056        ['activation_3[0][0]']           \n",
      "                                                                                                  \n",
      " bn3a_branch2a (BatchNormalizat  (None, 13, 13, 32)  128         ['res3a_branch2a[0][0]']         \n",
      " ion)                                                                                             \n",
      "                                                                                                  \n",
      " activation_4 (Activation)      (None, 13, 13, 32)   0           ['bn3a_branch2a[0][0]']          \n",
      "                                                                                                  \n",
      " res3a_branch2b (Conv2D)        (None, 13, 13, 32)   9248        ['activation_4[0][0]']           \n",
      "                                                                                                  \n",
      " bn3a_branch2b (BatchNormalizat  (None, 13, 13, 32)  128         ['res3a_branch2b[0][0]']         \n",
      " ion)                                                                                             \n",
      "                                                                                                  \n",
      " activation_5 (Activation)      (None, 13, 13, 32)   0           ['bn3a_branch2b[0][0]']          \n",
      "                                                                                                  \n",
      " res3a_branch2c (Conv2D)        (None, 13, 13, 64)   2112        ['activation_5[0][0]']           \n",
      "                                                                                                  \n",
      " res3a_branch1 (Conv2D)         (None, 13, 13, 64)   2112        ['activation_3[0][0]']           \n",
      "                                                                                                  \n",
      " bn3a_branch2c (BatchNormalizat  (None, 13, 13, 64)  256         ['res3a_branch2c[0][0]']         \n",
      " ion)                                                                                             \n",
      "                                                                                                  \n",
      " bn3a_branch1 (BatchNormalizati  (None, 13, 13, 64)  256         ['res3a_branch1[0][0]']          \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " add_1 (Add)                    (None, 13, 13, 64)   0           ['bn3a_branch2c[0][0]',          \n",
      "                                                                  'bn3a_branch1[0][0]']           \n",
      "                                                                                                  \n",
      " activation_6 (Activation)      (None, 13, 13, 64)   0           ['add_1[0][0]']                  \n",
      "                                                                                                  \n",
      " res4a_branch2a (Conv2D)        (None, 7, 7, 64)     4160        ['activation_6[0][0]']           \n",
      "                                                                                                  \n",
      " bn4a_branch2a (BatchNormalizat  (None, 7, 7, 64)    256         ['res4a_branch2a[0][0]']         \n",
      " ion)                                                                                             \n",
      "                                                                                                  \n",
      " activation_7 (Activation)      (None, 7, 7, 64)     0           ['bn4a_branch2a[0][0]']          \n",
      "                                                                                                  \n",
      " res4a_branch2b (Conv2D)        (None, 7, 7, 64)     36928       ['activation_7[0][0]']           \n",
      "                                                                                                  \n",
      " bn4a_branch2b (BatchNormalizat  (None, 7, 7, 64)    256         ['res4a_branch2b[0][0]']         \n",
      " ion)                                                                                             \n",
      "                                                                                                  \n",
      " activation_8 (Activation)      (None, 7, 7, 64)     0           ['bn4a_branch2b[0][0]']          \n",
      "                                                                                                  \n",
      " res4a_branch2c (Conv2D)        (None, 7, 7, 128)    8320        ['activation_8[0][0]']           \n",
      "                                                                                                  \n",
      " res4a_branch1 (Conv2D)         (None, 7, 7, 128)    8320        ['activation_6[0][0]']           \n",
      "                                                                                                  \n",
      " bn4a_branch2c (BatchNormalizat  (None, 7, 7, 128)   512         ['res4a_branch2c[0][0]']         \n",
      " ion)                                                                                             \n",
      "                                                                                                  \n",
      " bn4a_branch1 (BatchNormalizati  (None, 7, 7, 128)   512         ['res4a_branch1[0][0]']          \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " add_2 (Add)                    (None, 7, 7, 128)    0           ['bn4a_branch2c[0][0]',          \n",
      "                                                                  'bn4a_branch1[0][0]']           \n",
      "                                                                                                  \n",
      " activation_9 (Activation)      (None, 7, 7, 128)    0           ['add_2[0][0]']                  \n",
      "                                                                                                  \n",
      " res5a_branch2a (Conv2D)        (None, 4, 4, 128)    16512       ['activation_9[0][0]']           \n",
      "                                                                                                  \n",
      " bn5a_branch2a (BatchNormalizat  (None, 4, 4, 128)   512         ['res5a_branch2a[0][0]']         \n",
      " ion)                                                                                             \n",
      "                                                                                                  \n",
      " activation_10 (Activation)     (None, 4, 4, 128)    0           ['bn5a_branch2a[0][0]']          \n",
      "                                                                                                  \n",
      " res5a_branch2b (Conv2D)        (None, 4, 4, 128)    147584      ['activation_10[0][0]']          \n",
      "                                                                                                  \n",
      " bn5a_branch2b (BatchNormalizat  (None, 4, 4, 128)   512         ['res5a_branch2b[0][0]']         \n",
      " ion)                                                                                             \n",
      "                                                                                                  \n",
      " activation_11 (Activation)     (None, 4, 4, 128)    0           ['bn5a_branch2b[0][0]']          \n",
      "                                                                                                  \n",
      " res5a_branch2c (Conv2D)        (None, 4, 4, 256)    33024       ['activation_11[0][0]']          \n",
      "                                                                                                  \n",
      " res5a_branch1 (Conv2D)         (None, 4, 4, 256)    33024       ['activation_9[0][0]']           \n",
      "                                                                                                  \n",
      " bn5a_branch2c (BatchNormalizat  (None, 4, 4, 256)   1024        ['res5a_branch2c[0][0]']         \n",
      " ion)                                                                                             \n",
      "                                                                                                  \n",
      " bn5a_branch1 (BatchNormalizati  (None, 4, 4, 256)   1024        ['res5a_branch1[0][0]']          \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " add_3 (Add)                    (None, 4, 4, 256)    0           ['bn5a_branch2c[0][0]',          \n",
      "                                                                  'bn5a_branch1[0][0]']           \n",
      "                                                                                                  \n",
      " activation_12 (Activation)     (None, 4, 4, 256)    0           ['add_3[0][0]']                  \n",
      "                                                                                                  \n",
      " res6a_branch2a (Conv2D)        (None, 2, 2, 256)    65792       ['activation_12[0][0]']          \n",
      "                                                                                                  \n",
      " bn6a_branch2a (BatchNormalizat  (None, 2, 2, 256)   1024        ['res6a_branch2a[0][0]']         \n",
      " ion)                                                                                             \n",
      "                                                                                                  \n",
      " activation_13 (Activation)     (None, 2, 2, 256)    0           ['bn6a_branch2a[0][0]']          \n",
      "                                                                                                  \n",
      " res6a_branch2b (Conv2D)        (None, 2, 2, 256)    590080      ['activation_13[0][0]']          \n",
      "                                                                                                  \n",
      " bn6a_branch2b (BatchNormalizat  (None, 2, 2, 256)   1024        ['res6a_branch2b[0][0]']         \n",
      " ion)                                                                                             \n",
      "                                                                                                  \n",
      " activation_14 (Activation)     (None, 2, 2, 256)    0           ['bn6a_branch2b[0][0]']          \n",
      "                                                                                                  \n",
      " res6a_branch2c (Conv2D)        (None, 2, 2, 512)    131584      ['activation_14[0][0]']          \n",
      "                                                                                                  \n",
      " res6a_branch1 (Conv2D)         (None, 2, 2, 512)    131584      ['activation_12[0][0]']          \n",
      "                                                                                                  \n",
      " bn6a_branch2c (BatchNormalizat  (None, 2, 2, 512)   2048        ['res6a_branch2c[0][0]']         \n",
      " ion)                                                                                             \n",
      "                                                                                                  \n",
      " bn6a_branch1 (BatchNormalizati  (None, 2, 2, 512)   2048        ['res6a_branch1[0][0]']          \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " add_4 (Add)                    (None, 2, 2, 512)    0           ['bn6a_branch2c[0][0]',          \n",
      "                                                                  'bn6a_branch1[0][0]']           \n",
      "                                                                                                  \n",
      " activation_15 (Activation)     (None, 2, 2, 512)    0           ['add_4[0][0]']                  \n",
      "                                                                                                  \n",
      " avg_pool (AveragePooling2D)    (None, 1, 1, 512)    0           ['activation_15[0][0]']          \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 512)          0           ['avg_pool[0][0]']               \n",
      "                                                                                                  \n",
      " fc11 (Dense)                   (None, 11)           5643        ['flatten[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1,245,547\n",
      "Trainable params: 1,239,467\n",
      "Non-trainable params: 6,080\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "7/7 [==============================] - 3s 122ms/step - loss: 0.0491 - accuracy: 0.0250\n",
      "Epoch 2/500\n",
      "7/7 [==============================] - 1s 135ms/step - loss: 3.0676e-04 - accuracy: 0.2600\n",
      "Epoch 3/500\n",
      "7/7 [==============================] - 1s 121ms/step - loss: 7.9029e-05 - accuracy: 0.2900\n",
      "Epoch 4/500\n",
      "7/7 [==============================] - 1s 147ms/step - loss: 7.6173e-05 - accuracy: 0.4100\n",
      "Epoch 5/500\n",
      "7/7 [==============================] - 1s 140ms/step - loss: 7.5417e-05 - accuracy: 0.4800\n",
      "Epoch 6/500\n",
      "7/7 [==============================] - 1s 116ms/step - loss: 7.0836e-05 - accuracy: 0.5450\n",
      "Epoch 7/500\n",
      "7/7 [==============================] - 1s 133ms/step - loss: 6.3475e-05 - accuracy: 0.5850\n",
      "Epoch 8/500\n",
      "7/7 [==============================] - 1s 146ms/step - loss: 5.5937e-05 - accuracy: 0.5800\n",
      "Epoch 9/500\n",
      "7/7 [==============================] - 1s 118ms/step - loss: 5.0702e-05 - accuracy: 0.5750\n",
      "Epoch 10/500\n",
      "7/7 [==============================] - 1s 126ms/step - loss: 4.7548e-05 - accuracy: 0.5900\n",
      "Epoch 11/500\n",
      "7/7 [==============================] - 1s 144ms/step - loss: 4.3017e-05 - accuracy: 0.6200\n",
      "Epoch 12/500\n",
      "7/7 [==============================] - 1s 123ms/step - loss: 3.9689e-05 - accuracy: 0.6350\n",
      "Epoch 13/500\n",
      "7/7 [==============================] - 1s 131ms/step - loss: 3.6719e-05 - accuracy: 0.6300\n",
      "Epoch 14/500\n",
      "7/7 [==============================] - 1s 146ms/step - loss: 3.4268e-05 - accuracy: 0.6350\n",
      "Epoch 15/500\n",
      "7/7 [==============================] - 1s 123ms/step - loss: 3.1549e-05 - accuracy: 0.6350\n",
      "Epoch 16/500\n",
      "7/7 [==============================] - 1s 135ms/step - loss: 2.8775e-05 - accuracy: 0.6400\n",
      "Epoch 17/500\n",
      "7/7 [==============================] - 1s 146ms/step - loss: 2.6406e-05 - accuracy: 0.6400\n",
      "Epoch 18/500\n",
      "7/7 [==============================] - 1s 129ms/step - loss: 2.4461e-05 - accuracy: 0.6450\n",
      "Epoch 19/500\n",
      "7/7 [==============================] - 1s 147ms/step - loss: 2.4047e-05 - accuracy: 0.6450\n",
      "Epoch 20/500\n",
      "7/7 [==============================] - 1s 158ms/step - loss: 2.3245e-05 - accuracy: 0.6550\n",
      "Epoch 21/500\n",
      "7/7 [==============================] - 1s 122ms/step - loss: 2.2306e-05 - accuracy: 0.6500\n",
      "Epoch 22/500\n",
      "7/7 [==============================] - 1s 136ms/step - loss: 2.1479e-05 - accuracy: 0.6550\n",
      "Epoch 23/500\n",
      "7/7 [==============================] - 1s 146ms/step - loss: 2.0841e-05 - accuracy: 0.6550\n",
      "Epoch 24/500\n",
      "7/7 [==============================] - 1s 121ms/step - loss: 2.1177e-05 - accuracy: 0.6600\n",
      "Epoch 25/500\n",
      "7/7 [==============================] - 1s 140ms/step - loss: 1.9718e-05 - accuracy: 0.6550\n",
      "Epoch 26/500\n",
      "7/7 [==============================] - 1s 145ms/step - loss: 1.9792e-05 - accuracy: 0.6550\n",
      "Epoch 27/500\n",
      "7/7 [==============================] - 1s 126ms/step - loss: 1.9831e-05 - accuracy: 0.6550\n",
      "Epoch 28/500\n",
      "7/7 [==============================] - 1s 140ms/step - loss: 1.9390e-05 - accuracy: 0.6650\n",
      "Epoch 29/500\n",
      "7/7 [==============================] - 1s 138ms/step - loss: 1.8369e-05 - accuracy: 0.6600\n",
      "Epoch 30/500\n",
      "7/7 [==============================] - 1s 121ms/step - loss: 1.7399e-05 - accuracy: 0.6700\n",
      "Epoch 31/500\n",
      "7/7 [==============================] - 1s 133ms/step - loss: 1.8223e-05 - accuracy: 0.6600\n",
      "Epoch 32/500\n",
      "7/7 [==============================] - 1s 141ms/step - loss: 1.7370e-05 - accuracy: 0.6650\n",
      "Epoch 33/500\n",
      "7/7 [==============================] - 1s 125ms/step - loss: 1.7146e-05 - accuracy: 0.6700\n",
      "Epoch 34/500\n",
      "7/7 [==============================] - 1s 125ms/step - loss: 1.6545e-05 - accuracy: 0.6650\n",
      "Epoch 35/500\n",
      "7/7 [==============================] - 1s 148ms/step - loss: 1.5714e-05 - accuracy: 0.6700\n",
      "Epoch 36/500\n",
      "7/7 [==============================] - 1s 131ms/step - loss: 1.5517e-05 - accuracy: 0.6950\n",
      "Epoch 37/500\n",
      "7/7 [==============================] - 1s 129ms/step - loss: 1.5655e-05 - accuracy: 0.6700\n",
      "Epoch 38/500\n",
      "7/7 [==============================] - 1s 157ms/step - loss: 1.4832e-05 - accuracy: 0.6800\n",
      "Epoch 39/500\n",
      "7/7 [==============================] - 1s 130ms/step - loss: 1.4948e-05 - accuracy: 0.6850\n",
      "Epoch 40/500\n",
      "7/7 [==============================] - 1s 129ms/step - loss: 1.4456e-05 - accuracy: 0.7000\n",
      "Epoch 41/500\n",
      "7/7 [==============================] - 1s 163ms/step - loss: 1.4235e-05 - accuracy: 0.6850\n",
      "Epoch 42/500\n",
      "7/7 [==============================] - 1s 125ms/step - loss: 1.3667e-05 - accuracy: 0.6850\n",
      "Epoch 43/500\n",
      "7/7 [==============================] - 1s 122ms/step - loss: 1.3124e-05 - accuracy: 0.6900\n",
      "Epoch 44/500\n",
      "7/7 [==============================] - 1s 148ms/step - loss: 1.4076e-05 - accuracy: 0.7000\n",
      "Epoch 45/500\n",
      "7/7 [==============================] - 1s 123ms/step - loss: 1.3971e-05 - accuracy: 0.6950\n",
      "Epoch 46/500\n",
      "7/7 [==============================] - 1s 119ms/step - loss: 1.3275e-05 - accuracy: 0.7350\n",
      "Epoch 47/500\n",
      "7/7 [==============================] - 1s 157ms/step - loss: 1.3450e-05 - accuracy: 0.7200\n",
      "Epoch 48/500\n",
      "7/7 [==============================] - 1s 131ms/step - loss: 1.2068e-05 - accuracy: 0.7200\n",
      "Epoch 49/500\n",
      "7/7 [==============================] - 1s 118ms/step - loss: 1.2169e-05 - accuracy: 0.7300\n",
      "Epoch 50/500\n",
      "7/7 [==============================] - 1s 152ms/step - loss: 1.2217e-05 - accuracy: 0.7250\n",
      "Epoch 51/500\n",
      "7/7 [==============================] - 1s 126ms/step - loss: 1.1680e-05 - accuracy: 0.7300\n",
      "Epoch 52/500\n",
      "7/7 [==============================] - 1s 134ms/step - loss: 1.2525e-05 - accuracy: 0.7450\n",
      "Epoch 53/500\n",
      "7/7 [==============================] - 1s 169ms/step - loss: 1.1106e-05 - accuracy: 0.7300\n",
      "Epoch 54/500\n",
      "7/7 [==============================] - 1s 124ms/step - loss: 1.2540e-05 - accuracy: 0.7150\n",
      "Epoch 55/500\n",
      "7/7 [==============================] - 1s 126ms/step - loss: 1.1088e-05 - accuracy: 0.7800\n",
      "Epoch 56/500\n",
      "7/7 [==============================] - 1s 147ms/step - loss: 1.1486e-05 - accuracy: 0.7600\n",
      "Epoch 57/500\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 1.0907e-05 - accuracy: 0.7550\n",
      "Epoch 58/500\n",
      "7/7 [==============================] - 1s 120ms/step - loss: 1.1034e-05 - accuracy: 0.7650\n",
      "Epoch 59/500\n",
      "7/7 [==============================] - 1s 140ms/step - loss: 1.0219e-05 - accuracy: 0.7400\n",
      "Epoch 60/500\n",
      "7/7 [==============================] - 1s 135ms/step - loss: 1.0918e-05 - accuracy: 0.7550\n",
      "Epoch 61/500\n",
      "7/7 [==============================] - 1s 119ms/step - loss: 1.0435e-05 - accuracy: 0.7550\n",
      "Epoch 62/500\n",
      "7/7 [==============================] - 1s 145ms/step - loss: 1.0474e-05 - accuracy: 0.7400\n",
      "Epoch 63/500\n",
      "7/7 [==============================] - 1s 142ms/step - loss: 8.9496e-06 - accuracy: 0.7900\n",
      "Epoch 64/500\n",
      "7/7 [==============================] - 1s 116ms/step - loss: 8.7191e-06 - accuracy: 0.8050\n",
      "Epoch 65/500\n",
      "7/7 [==============================] - 1s 153ms/step - loss: 9.0249e-06 - accuracy: 0.8200\n",
      "Epoch 66/500\n",
      "7/7 [==============================] - 1s 140ms/step - loss: 9.5451e-06 - accuracy: 0.7600\n",
      "Epoch 67/500\n",
      "7/7 [==============================] - 1s 120ms/step - loss: 9.3756e-06 - accuracy: 0.7950\n",
      "Epoch 68/500\n",
      "7/7 [==============================] - 1s 146ms/step - loss: 9.8091e-06 - accuracy: 0.7800\n",
      "Epoch 69/500\n",
      "7/7 [==============================] - 1s 140ms/step - loss: 8.8346e-06 - accuracy: 0.8100\n",
      "Epoch 70/500\n",
      "7/7 [==============================] - 1s 118ms/step - loss: 9.4049e-06 - accuracy: 0.7950\n",
      "Epoch 71/500\n",
      "7/7 [==============================] - 1s 136ms/step - loss: 9.3329e-06 - accuracy: 0.8000\n",
      "Epoch 72/500\n",
      "7/7 [==============================] - 1s 148ms/step - loss: 8.9265e-06 - accuracy: 0.7950\n",
      "Epoch 73/500\n",
      "7/7 [==============================] - 1s 117ms/step - loss: 8.0383e-06 - accuracy: 0.8000\n",
      "Epoch 74/500\n",
      "7/7 [==============================] - 1s 131ms/step - loss: 8.5434e-06 - accuracy: 0.8150\n",
      "Epoch 75/500\n",
      "7/7 [==============================] - 1s 151ms/step - loss: 8.2716e-06 - accuracy: 0.8200\n",
      "Epoch 76/500\n",
      "7/7 [==============================] - 1s 115ms/step - loss: 7.4150e-06 - accuracy: 0.8450\n",
      "Epoch 77/500\n",
      "7/7 [==============================] - 1s 129ms/step - loss: 7.7377e-06 - accuracy: 0.8250\n",
      "Epoch 78/500\n",
      "7/7 [==============================] - 1s 157ms/step - loss: 7.7949e-06 - accuracy: 0.8200\n",
      "Epoch 79/500\n",
      "7/7 [==============================] - 1s 117ms/step - loss: 7.2901e-06 - accuracy: 0.8200\n",
      "Epoch 80/500\n",
      "7/7 [==============================] - 1s 129ms/step - loss: 7.9292e-06 - accuracy: 0.8350\n",
      "Epoch 81/500\n",
      "7/7 [==============================] - 1s 152ms/step - loss: 8.4486e-06 - accuracy: 0.7950\n",
      "Epoch 82/500\n",
      "7/7 [==============================] - 1s 120ms/step - loss: 9.0986e-06 - accuracy: 0.8350\n",
      "Epoch 83/500\n",
      "7/7 [==============================] - 1s 136ms/step - loss: 7.4112e-06 - accuracy: 0.8350\n",
      "Epoch 84/500\n",
      "7/7 [==============================] - 1s 150ms/step - loss: 6.4139e-06 - accuracy: 0.8550\n",
      "Epoch 85/500\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 7.7341e-06 - accuracy: 0.8350\n",
      "Epoch 86/500\n",
      "7/7 [==============================] - 1s 133ms/step - loss: 7.7053e-06 - accuracy: 0.8350\n",
      "Epoch 87/500\n",
      "7/7 [==============================] - 1s 146ms/step - loss: 6.6900e-06 - accuracy: 0.8850\n",
      "Epoch 88/500\n",
      "7/7 [==============================] - 1s 125ms/step - loss: 8.2132e-06 - accuracy: 0.8300\n",
      "Epoch 89/500\n",
      "7/7 [==============================] - 1s 124ms/step - loss: 6.5585e-06 - accuracy: 0.8600\n",
      "Epoch 90/500\n",
      "7/7 [==============================] - 1s 144ms/step - loss: 6.7676e-06 - accuracy: 0.8550\n",
      "Epoch 91/500\n",
      "7/7 [==============================] - 1s 126ms/step - loss: 6.3680e-06 - accuracy: 0.8600\n",
      "Epoch 92/500\n",
      "7/7 [==============================] - 1s 120ms/step - loss: 7.0319e-06 - accuracy: 0.8500\n",
      "Epoch 93/500\n",
      "7/7 [==============================] - 1s 161ms/step - loss: 6.8437e-06 - accuracy: 0.8550\n",
      "Epoch 94/500\n",
      "7/7 [==============================] - 1s 130ms/step - loss: 6.5428e-06 - accuracy: 0.8550\n",
      "Epoch 95/500\n",
      "7/7 [==============================] - 1s 129ms/step - loss: 6.4797e-06 - accuracy: 0.8700\n",
      "Epoch 96/500\n",
      "7/7 [==============================] - 1s 152ms/step - loss: 6.5065e-06 - accuracy: 0.8600\n",
      "Epoch 97/500\n",
      "7/7 [==============================] - 1s 135ms/step - loss: 6.6981e-06 - accuracy: 0.8450\n",
      "Epoch 98/500\n",
      "7/7 [==============================] - 1s 135ms/step - loss: 6.4737e-06 - accuracy: 0.8750\n",
      "Epoch 99/500\n",
      "7/7 [==============================] - 1s 151ms/step - loss: 7.1832e-06 - accuracy: 0.8650\n",
      "Epoch 100/500\n",
      "7/7 [==============================] - 1s 131ms/step - loss: 6.1518e-06 - accuracy: 0.8650\n",
      "Epoch 101/500\n",
      "7/7 [==============================] - 1s 130ms/step - loss: 8.9262e-06 - accuracy: 0.8300\n",
      "Epoch 102/500\n",
      "7/7 [==============================] - 1s 153ms/step - loss: 6.3714e-06 - accuracy: 0.8550\n",
      "Epoch 103/500\n",
      "7/7 [==============================] - 1s 132ms/step - loss: 6.1070e-06 - accuracy: 0.8600\n",
      "Epoch 104/500\n",
      "7/7 [==============================] - 1s 132ms/step - loss: 6.3494e-06 - accuracy: 0.8850\n",
      "Epoch 105/500\n",
      "7/7 [==============================] - 1s 151ms/step - loss: 6.4344e-06 - accuracy: 0.8900\n",
      "Epoch 106/500\n",
      "7/7 [==============================] - 1s 125ms/step - loss: 6.6536e-06 - accuracy: 0.8950\n",
      "Epoch 107/500\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 5.7145e-06 - accuracy: 0.8550\n",
      "Epoch 108/500\n",
      "7/7 [==============================] - 1s 151ms/step - loss: 6.3525e-06 - accuracy: 0.8750\n",
      "Epoch 109/500\n",
      "7/7 [==============================] - 1s 129ms/step - loss: 5.9627e-06 - accuracy: 0.8850\n",
      "Epoch 110/500\n",
      "7/7 [==============================] - 1s 124ms/step - loss: 6.2776e-06 - accuracy: 0.8450\n",
      "Epoch 111/500\n",
      "7/7 [==============================] - 1s 141ms/step - loss: 6.5345e-06 - accuracy: 0.9050\n",
      "Epoch 112/500\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 6.0284e-06 - accuracy: 0.8700\n",
      "Epoch 113/500\n",
      "7/7 [==============================] - 1s 130ms/step - loss: 5.4172e-06 - accuracy: 0.8650\n",
      "Epoch 114/500\n",
      "7/7 [==============================] - 1s 144ms/step - loss: 6.1257e-06 - accuracy: 0.8750\n",
      "Epoch 115/500\n",
      "7/7 [==============================] - 1s 130ms/step - loss: 5.9558e-06 - accuracy: 0.8600\n",
      "Epoch 116/500\n",
      "7/7 [==============================] - 1s 123ms/step - loss: 6.3426e-06 - accuracy: 0.8950\n",
      "Epoch 117/500\n",
      "7/7 [==============================] - 1s 136ms/step - loss: 6.7095e-06 - accuracy: 0.8500\n",
      "Epoch 118/500\n",
      "7/7 [==============================] - 1s 130ms/step - loss: 6.8041e-06 - accuracy: 0.8800\n",
      "Epoch 119/500\n",
      "7/7 [==============================] - 1s 122ms/step - loss: 5.8528e-06 - accuracy: 0.8500\n",
      "Epoch 120/500\n",
      "7/7 [==============================] - 1s 135ms/step - loss: 6.0789e-06 - accuracy: 0.8950\n",
      "Epoch 121/500\n",
      "7/7 [==============================] - 1s 137ms/step - loss: 7.3102e-06 - accuracy: 0.8400\n",
      "Epoch 122/500\n",
      "7/7 [==============================] - 1s 118ms/step - loss: 6.5953e-06 - accuracy: 0.8750\n",
      "Epoch 123/500\n",
      "7/7 [==============================] - 1s 147ms/step - loss: 6.5834e-06 - accuracy: 0.8800\n",
      "Epoch 124/500\n",
      "7/7 [==============================] - 1s 138ms/step - loss: 7.1198e-06 - accuracy: 0.8800\n",
      "Epoch 125/500\n",
      "7/7 [==============================] - 1s 120ms/step - loss: 6.7565e-06 - accuracy: 0.8650\n",
      "Epoch 126/500\n",
      "7/7 [==============================] - 1s 154ms/step - loss: 6.0938e-06 - accuracy: 0.8850\n",
      "Epoch 127/500\n",
      "7/7 [==============================] - 1s 142ms/step - loss: 5.7758e-06 - accuracy: 0.8550\n",
      "Epoch 128/500\n",
      "7/7 [==============================] - 1s 117ms/step - loss: 5.2258e-06 - accuracy: 0.8550\n",
      "Epoch 129/500\n",
      "7/7 [==============================] - 1s 144ms/step - loss: 4.9168e-06 - accuracy: 0.8950\n",
      "Epoch 130/500\n",
      "7/7 [==============================] - 1s 150ms/step - loss: 5.4972e-06 - accuracy: 0.8850\n",
      "Epoch 131/500\n",
      "7/7 [==============================] - 1s 123ms/step - loss: 5.1379e-06 - accuracy: 0.8750\n",
      "Epoch 132/500\n",
      "7/7 [==============================] - 1s 148ms/step - loss: 4.9639e-06 - accuracy: 0.9100\n",
      "Epoch 133/500\n",
      "7/7 [==============================] - 1s 141ms/step - loss: 5.2242e-06 - accuracy: 0.8950\n",
      "Epoch 134/500\n",
      "7/7 [==============================] - 1s 117ms/step - loss: 5.5553e-06 - accuracy: 0.9000\n",
      "Epoch 135/500\n",
      "7/7 [==============================] - 1s 149ms/step - loss: 5.0008e-06 - accuracy: 0.9250\n",
      "Epoch 136/500\n",
      "7/7 [==============================] - 1s 135ms/step - loss: 4.7316e-06 - accuracy: 0.9200\n",
      "Epoch 137/500\n",
      "7/7 [==============================] - 1s 117ms/step - loss: 5.2074e-06 - accuracy: 0.9250\n",
      "Epoch 138/500\n",
      "7/7 [==============================] - 1s 151ms/step - loss: 5.1037e-06 - accuracy: 0.9200\n",
      "Epoch 139/500\n",
      "7/7 [==============================] - 1s 134ms/step - loss: 5.6385e-06 - accuracy: 0.8800\n",
      "Epoch 140/500\n",
      "7/7 [==============================] - 1s 120ms/step - loss: 5.4347e-06 - accuracy: 0.8650\n",
      "Epoch 141/500\n",
      "7/7 [==============================] - 1s 141ms/step - loss: 5.1451e-06 - accuracy: 0.8900\n",
      "Epoch 142/500\n",
      "7/7 [==============================] - 1s 144ms/step - loss: 5.4993e-06 - accuracy: 0.9250\n",
      "Epoch 143/500\n",
      "7/7 [==============================] - 1s 118ms/step - loss: 5.3401e-06 - accuracy: 0.8850\n",
      "Epoch 144/500\n",
      "7/7 [==============================] - 1s 131ms/step - loss: 4.9287e-06 - accuracy: 0.8750\n",
      "Epoch 145/500\n",
      "7/7 [==============================] - 1s 154ms/step - loss: 4.7821e-06 - accuracy: 0.8900\n",
      "Epoch 146/500\n",
      "7/7 [==============================] - 1s 122ms/step - loss: 5.6700e-06 - accuracy: 0.8800\n",
      "Epoch 147/500\n",
      "7/7 [==============================] - 1s 135ms/step - loss: 5.2191e-06 - accuracy: 0.9000\n",
      "Epoch 148/500\n",
      "7/7 [==============================] - 1s 159ms/step - loss: 4.4209e-06 - accuracy: 0.9150\n",
      "Epoch 149/500\n",
      "7/7 [==============================] - 1s 130ms/step - loss: 4.9006e-06 - accuracy: 0.9400\n",
      "Epoch 150/500\n",
      "7/7 [==============================] - 1s 140ms/step - loss: 5.7860e-06 - accuracy: 0.9000\n",
      "Epoch 151/500\n",
      "7/7 [==============================] - 1s 144ms/step - loss: 4.9339e-06 - accuracy: 0.8950\n",
      "Epoch 152/500\n",
      "7/7 [==============================] - 1s 120ms/step - loss: 5.7829e-06 - accuracy: 0.9050\n",
      "Epoch 153/500\n",
      "7/7 [==============================] - 1s 137ms/step - loss: 4.3416e-06 - accuracy: 0.9100\n",
      "Epoch 154/500\n",
      "7/7 [==============================] - 1s 137ms/step - loss: 4.8284e-06 - accuracy: 0.9250\n",
      "Epoch 155/500\n",
      "7/7 [==============================] - 1s 123ms/step - loss: 4.8574e-06 - accuracy: 0.8600\n",
      "Epoch 156/500\n",
      "7/7 [==============================] - 1s 134ms/step - loss: 5.3079e-06 - accuracy: 0.9000\n",
      "Epoch 157/500\n",
      "7/7 [==============================] - 1s 139ms/step - loss: 6.4887e-06 - accuracy: 0.8850\n",
      "Epoch 158/500\n",
      "7/7 [==============================] - 1s 125ms/step - loss: 5.7338e-06 - accuracy: 0.9000\n",
      "Epoch 159/500\n",
      "7/7 [==============================] - 1s 142ms/step - loss: 5.8539e-06 - accuracy: 0.8600\n",
      "Epoch 160/500\n",
      "7/7 [==============================] - 1s 156ms/step - loss: 7.0875e-06 - accuracy: 0.8650\n",
      "Epoch 161/500\n",
      "7/7 [==============================] - 1s 125ms/step - loss: 5.2157e-06 - accuracy: 0.8700\n",
      "Epoch 162/500\n",
      "7/7 [==============================] - 1s 133ms/step - loss: 5.4706e-06 - accuracy: 0.8900\n",
      "Epoch 163/500\n",
      "7/7 [==============================] - 1s 165ms/step - loss: 5.2237e-06 - accuracy: 0.8800\n",
      "Epoch 164/500\n",
      "7/7 [==============================] - 1s 131ms/step - loss: 4.7404e-06 - accuracy: 0.8950\n",
      "Epoch 165/500\n",
      "7/7 [==============================] - 1s 133ms/step - loss: 4.6539e-06 - accuracy: 0.9000\n",
      "Epoch 166/500\n",
      "7/7 [==============================] - 1s 155ms/step - loss: 4.9894e-06 - accuracy: 0.8750\n",
      "Epoch 167/500\n",
      "7/7 [==============================] - 1s 124ms/step - loss: 4.8023e-06 - accuracy: 0.8900\n",
      "Epoch 168/500\n",
      "7/7 [==============================] - 1s 130ms/step - loss: 4.4773e-06 - accuracy: 0.9150\n",
      "Epoch 169/500\n",
      "7/7 [==============================] - 1s 141ms/step - loss: 4.4420e-06 - accuracy: 0.9150\n",
      "Epoch 170/500\n",
      "7/7 [==============================] - 1s 124ms/step - loss: 5.5855e-06 - accuracy: 0.8850\n",
      "Epoch 171/500\n",
      "7/7 [==============================] - 1s 136ms/step - loss: 4.8741e-06 - accuracy: 0.9100\n",
      "Epoch 172/500\n",
      "7/7 [==============================] - 1s 142ms/step - loss: 5.3268e-06 - accuracy: 0.9100\n",
      "Epoch 173/500\n",
      "7/7 [==============================] - 1s 124ms/step - loss: 4.4464e-06 - accuracy: 0.9100\n",
      "Epoch 174/500\n",
      "7/7 [==============================] - 1s 135ms/step - loss: 4.8667e-06 - accuracy: 0.9000\n",
      "Epoch 175/500\n",
      "7/7 [==============================] - 1s 144ms/step - loss: 4.2448e-06 - accuracy: 0.9050\n",
      "Epoch 176/500\n",
      "7/7 [==============================] - 1s 125ms/step - loss: 4.6959e-06 - accuracy: 0.9150\n",
      "Epoch 177/500\n",
      "7/7 [==============================] - 1s 129ms/step - loss: 4.6503e-06 - accuracy: 0.9500\n",
      "Epoch 178/500\n",
      "7/7 [==============================] - 1s 153ms/step - loss: 4.4895e-06 - accuracy: 0.9150\n",
      "Epoch 179/500\n",
      "7/7 [==============================] - 1s 126ms/step - loss: 4.4699e-06 - accuracy: 0.8950\n",
      "Epoch 180/500\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 5.3889e-06 - accuracy: 0.9150\n",
      "Epoch 181/500\n",
      "7/7 [==============================] - 1s 150ms/step - loss: 6.0266e-06 - accuracy: 0.8600\n",
      "Epoch 182/500\n",
      "7/7 [==============================] - 1s 135ms/step - loss: 5.1230e-06 - accuracy: 0.9000\n",
      "Epoch 183/500\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 4.9694e-06 - accuracy: 0.9400\n",
      "Epoch 184/500\n",
      "7/7 [==============================] - 1s 153ms/step - loss: 4.7989e-06 - accuracy: 0.9050\n",
      "Epoch 185/500\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 4.3366e-06 - accuracy: 0.9000\n",
      "Epoch 186/500\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 4.7607e-06 - accuracy: 0.8950\n",
      "Epoch 187/500\n",
      "7/7 [==============================] - 1s 144ms/step - loss: 4.6462e-06 - accuracy: 0.8900\n",
      "Epoch 188/500\n",
      "7/7 [==============================] - 1s 125ms/step - loss: 5.0034e-06 - accuracy: 0.9000\n",
      "Epoch 189/500\n",
      "7/7 [==============================] - 1s 129ms/step - loss: 4.4264e-06 - accuracy: 0.9100\n",
      "Epoch 190/500\n",
      "7/7 [==============================] - 1s 144ms/step - loss: 4.5619e-06 - accuracy: 0.9100\n",
      "Epoch 191/500\n",
      "7/7 [==============================] - 1s 122ms/step - loss: 4.5170e-06 - accuracy: 0.9400\n",
      "Epoch 192/500\n",
      "7/7 [==============================] - 1s 126ms/step - loss: 3.6291e-06 - accuracy: 0.9350\n",
      "Epoch 193/500\n",
      "7/7 [==============================] - 1s 144ms/step - loss: 4.2501e-06 - accuracy: 0.9600\n",
      "Epoch 194/500\n",
      "7/7 [==============================] - 1s 132ms/step - loss: 3.8138e-06 - accuracy: 0.9300\n",
      "Epoch 195/500\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 4.1438e-06 - accuracy: 0.9200\n",
      "Epoch 196/500\n",
      "7/7 [==============================] - 1s 131ms/step - loss: 4.2878e-06 - accuracy: 0.9300\n",
      "Epoch 197/500\n",
      "7/7 [==============================] - 1s 129ms/step - loss: 5.4383e-06 - accuracy: 0.9200\n",
      "Epoch 198/500\n",
      "7/7 [==============================] - 1s 130ms/step - loss: 5.4764e-06 - accuracy: 0.8900\n",
      "Epoch 199/500\n",
      "7/7 [==============================] - 1s 139ms/step - loss: 4.1415e-06 - accuracy: 0.9100\n",
      "Epoch 200/500\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 4.4905e-06 - accuracy: 0.9000\n",
      "Epoch 201/500\n",
      "7/7 [==============================] - 1s 126ms/step - loss: 6.6265e-06 - accuracy: 0.8750\n",
      "Epoch 202/500\n",
      "7/7 [==============================] - 1s 144ms/step - loss: 5.7956e-06 - accuracy: 0.9150\n",
      "Epoch 203/500\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 5.0358e-06 - accuracy: 0.9100\n",
      "Epoch 204/500\n",
      "7/7 [==============================] - 1s 125ms/step - loss: 4.6179e-06 - accuracy: 0.9150\n",
      "Epoch 205/500\n",
      "7/7 [==============================] - 1s 145ms/step - loss: 5.8242e-06 - accuracy: 0.8700\n",
      "Epoch 206/500\n",
      "7/7 [==============================] - 1s 136ms/step - loss: 4.9555e-06 - accuracy: 0.9050\n",
      "Epoch 207/500\n",
      "7/7 [==============================] - 1s 120ms/step - loss: 4.4222e-06 - accuracy: 0.9200\n",
      "Epoch 208/500\n",
      "7/7 [==============================] - 1s 143ms/step - loss: 4.4365e-06 - accuracy: 0.9000\n",
      "Epoch 209/500\n",
      "7/7 [==============================] - 1s 137ms/step - loss: 5.2199e-06 - accuracy: 0.9100\n",
      "Epoch 210/500\n",
      "7/7 [==============================] - 1s 122ms/step - loss: 4.0148e-06 - accuracy: 0.9350\n",
      "Epoch 211/500\n",
      "7/7 [==============================] - 1s 146ms/step - loss: 4.7168e-06 - accuracy: 0.9050\n",
      "Epoch 212/500\n",
      "7/7 [==============================] - 1s 133ms/step - loss: 4.6067e-06 - accuracy: 0.9000\n",
      "Epoch 213/500\n",
      "7/7 [==============================] - 1s 126ms/step - loss: 4.5033e-06 - accuracy: 0.9350\n",
      "Epoch 214/500\n",
      "7/7 [==============================] - 1s 167ms/step - loss: 4.1160e-06 - accuracy: 0.9050\n",
      "Epoch 215/500\n",
      "7/7 [==============================] - 1s 132ms/step - loss: 4.1267e-06 - accuracy: 0.9150\n",
      "Epoch 216/500\n",
      "7/7 [==============================] - 1s 120ms/step - loss: 5.6235e-06 - accuracy: 0.9200\n",
      "Epoch 217/500\n",
      "7/7 [==============================] - 1s 155ms/step - loss: 3.6037e-06 - accuracy: 0.9050\n",
      "Epoch 218/500\n",
      "7/7 [==============================] - 1s 134ms/step - loss: 4.2615e-06 - accuracy: 0.9200\n",
      "Epoch 219/500\n",
      "7/7 [==============================] - 1s 137ms/step - loss: 5.1219e-06 - accuracy: 0.8650\n",
      "Epoch 220/500\n",
      "7/7 [==============================] - 1s 153ms/step - loss: 3.8650e-06 - accuracy: 0.9150\n",
      "Epoch 221/500\n",
      "7/7 [==============================] - 1s 152ms/step - loss: 3.9965e-06 - accuracy: 0.9400\n",
      "Epoch 222/500\n",
      "7/7 [==============================] - 1s 142ms/step - loss: 4.1967e-06 - accuracy: 0.8900\n",
      "Epoch 223/500\n",
      "7/7 [==============================] - 1s 152ms/step - loss: 4.1307e-06 - accuracy: 0.9100\n",
      "Epoch 224/500\n",
      "7/7 [==============================] - 1s 136ms/step - loss: 4.5424e-06 - accuracy: 0.9100\n",
      "Epoch 225/500\n",
      "7/7 [==============================] - 1s 139ms/step - loss: 5.1554e-06 - accuracy: 0.8900\n",
      "Epoch 226/500\n",
      "7/7 [==============================] - 1s 177ms/step - loss: 5.5245e-06 - accuracy: 0.8600\n",
      "Epoch 227/500\n",
      "7/7 [==============================] - 1s 144ms/step - loss: 5.9271e-06 - accuracy: 0.8900\n",
      "Epoch 228/500\n",
      "7/7 [==============================] - 1s 141ms/step - loss: 4.2901e-06 - accuracy: 0.9200\n",
      "Epoch 229/500\n",
      "7/7 [==============================] - 1s 130ms/step - loss: 6.4767e-06 - accuracy: 0.8600\n",
      "Epoch 230/500\n",
      "7/7 [==============================] - 1s 134ms/step - loss: 4.5231e-06 - accuracy: 0.9250\n",
      "Epoch 231/500\n",
      "7/7 [==============================] - 1s 139ms/step - loss: 4.6100e-06 - accuracy: 0.9350\n",
      "Epoch 232/500\n",
      "7/7 [==============================] - 1s 141ms/step - loss: 3.6828e-06 - accuracy: 0.9250\n",
      "Epoch 233/500\n",
      "7/7 [==============================] - 1s 135ms/step - loss: 3.7502e-06 - accuracy: 0.9300\n",
      "Epoch 234/500\n",
      "7/7 [==============================] - 1s 141ms/step - loss: 3.5945e-06 - accuracy: 0.9100\n",
      "Epoch 235/500\n",
      "7/7 [==============================] - 1s 137ms/step - loss: 4.8829e-06 - accuracy: 0.8900\n",
      "Epoch 236/500\n",
      "7/7 [==============================] - 1s 132ms/step - loss: 3.8205e-06 - accuracy: 0.9350\n",
      "Epoch 237/500\n",
      "7/7 [==============================] - 1s 145ms/step - loss: 4.0654e-06 - accuracy: 0.9000\n",
      "Epoch 238/500\n",
      "7/7 [==============================] - 1s 139ms/step - loss: 5.0785e-06 - accuracy: 0.9150\n",
      "Epoch 239/500\n",
      "7/7 [==============================] - 1s 132ms/step - loss: 3.9114e-06 - accuracy: 0.9150\n",
      "Epoch 240/500\n",
      "7/7 [==============================] - 1s 136ms/step - loss: 3.7696e-06 - accuracy: 0.9200\n",
      "Epoch 241/500\n",
      "7/7 [==============================] - 1s 136ms/step - loss: 3.5007e-06 - accuracy: 0.8900\n",
      "Epoch 242/500\n",
      "7/7 [==============================] - 1s 130ms/step - loss: 3.2858e-06 - accuracy: 0.9300\n",
      "Epoch 243/500\n",
      "7/7 [==============================] - 1s 132ms/step - loss: 3.7715e-06 - accuracy: 0.9000\n",
      "Epoch 244/500\n",
      "7/7 [==============================] - 1s 140ms/step - loss: 3.4202e-06 - accuracy: 0.9000\n",
      "Epoch 245/500\n",
      "7/7 [==============================] - 1s 130ms/step - loss: 4.1203e-06 - accuracy: 0.9000\n",
      "Epoch 246/500\n",
      "7/7 [==============================] - 1s 134ms/step - loss: 3.6922e-06 - accuracy: 0.9200\n",
      "Epoch 247/500\n",
      "7/7 [==============================] - 1s 141ms/step - loss: 4.8861e-06 - accuracy: 0.8800\n",
      "Epoch 248/500\n",
      "7/7 [==============================] - 1s 130ms/step - loss: 4.9957e-06 - accuracy: 0.9200\n",
      "Epoch 249/500\n",
      "7/7 [==============================] - 1s 133ms/step - loss: 5.2018e-06 - accuracy: 0.9250\n",
      "Epoch 250/500\n",
      "7/7 [==============================] - 1s 132ms/step - loss: 5.8189e-06 - accuracy: 0.8850\n",
      "Epoch 251/500\n",
      "7/7 [==============================] - 1s 137ms/step - loss: 5.3464e-06 - accuracy: 0.8850\n",
      "Epoch 252/500\n",
      "7/7 [==============================] - 1s 131ms/step - loss: 4.2463e-06 - accuracy: 0.8950\n",
      "Epoch 253/500\n",
      "7/7 [==============================] - 1s 137ms/step - loss: 3.4409e-06 - accuracy: 0.9100\n",
      "Epoch 254/500\n",
      "7/7 [==============================] - 1s 142ms/step - loss: 3.2060e-06 - accuracy: 0.9250\n",
      "Epoch 255/500\n",
      "7/7 [==============================] - 1s 142ms/step - loss: 3.5613e-06 - accuracy: 0.9000\n",
      "Epoch 256/500\n",
      "7/7 [==============================] - 1s 147ms/step - loss: 4.2543e-06 - accuracy: 0.8800\n",
      "Epoch 257/500\n",
      "7/7 [==============================] - 1s 141ms/step - loss: 4.0290e-06 - accuracy: 0.9250\n",
      "Epoch 258/500\n",
      "7/7 [==============================] - 1s 139ms/step - loss: 3.6087e-06 - accuracy: 0.8950\n",
      "Epoch 259/500\n",
      "7/7 [==============================] - 1s 138ms/step - loss: 3.4524e-06 - accuracy: 0.8950\n",
      "Epoch 260/500\n",
      "7/7 [==============================] - 1s 134ms/step - loss: 3.8267e-06 - accuracy: 0.9350\n",
      "Epoch 261/500\n",
      "7/7 [==============================] - 1s 137ms/step - loss: 4.6379e-06 - accuracy: 0.9000\n",
      "Epoch 262/500\n",
      "7/7 [==============================] - 1s 146ms/step - loss: 4.5975e-06 - accuracy: 0.9100\n",
      "Epoch 263/500\n",
      "7/7 [==============================] - 1s 141ms/step - loss: 4.1603e-06 - accuracy: 0.9200\n",
      "Epoch 264/500\n",
      "7/7 [==============================] - 1s 131ms/step - loss: 4.0603e-06 - accuracy: 0.9250\n",
      "Epoch 265/500\n",
      "7/7 [==============================] - 1s 135ms/step - loss: 3.5185e-06 - accuracy: 0.9100\n",
      "Epoch 266/500\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 3.3097e-06 - accuracy: 0.9050\n",
      "Epoch 267/500\n",
      "7/7 [==============================] - 1s 133ms/step - loss: 4.6425e-06 - accuracy: 0.8900\n",
      "Epoch 268/500\n",
      "7/7 [==============================] - 1s 139ms/step - loss: 4.0806e-06 - accuracy: 0.9100\n",
      "Epoch 269/500\n",
      "7/7 [==============================] - 1s 136ms/step - loss: 3.6673e-06 - accuracy: 0.9250\n",
      "Epoch 270/500\n",
      "7/7 [==============================] - 1s 131ms/step - loss: 4.0031e-06 - accuracy: 0.8900\n",
      "Epoch 271/500\n",
      "7/7 [==============================] - 1s 138ms/step - loss: 3.3739e-06 - accuracy: 0.9300\n",
      "Epoch 272/500\n",
      "7/7 [==============================] - 1s 132ms/step - loss: 4.2823e-06 - accuracy: 0.9050\n",
      "Epoch 273/500\n",
      "7/7 [==============================] - 1s 126ms/step - loss: 3.8253e-06 - accuracy: 0.9100\n",
      "Epoch 274/500\n",
      "7/7 [==============================] - 1s 136ms/step - loss: 3.4675e-06 - accuracy: 0.9600\n",
      "Epoch 275/500\n",
      "7/7 [==============================] - 1s 132ms/step - loss: 4.2655e-06 - accuracy: 0.9150\n",
      "Epoch 276/500\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 3.1036e-06 - accuracy: 0.9250\n",
      "Epoch 277/500\n",
      "7/7 [==============================] - 1s 144ms/step - loss: 3.1741e-06 - accuracy: 0.9450\n",
      "Epoch 278/500\n",
      "7/7 [==============================] - 1s 137ms/step - loss: 3.3270e-06 - accuracy: 0.9250\n",
      "Epoch 279/500\n",
      "7/7 [==============================] - 1s 123ms/step - loss: 3.3759e-06 - accuracy: 0.9100\n",
      "Epoch 280/500\n",
      "7/7 [==============================] - 1s 135ms/step - loss: 3.4150e-06 - accuracy: 0.9350\n",
      "Epoch 281/500\n",
      "7/7 [==============================] - 1s 132ms/step - loss: 4.2773e-06 - accuracy: 0.8950\n",
      "Epoch 282/500\n",
      "7/7 [==============================] - 1s 131ms/step - loss: 3.2362e-06 - accuracy: 0.9200\n",
      "Epoch 283/500\n",
      "7/7 [==============================] - 1s 133ms/step - loss: 3.4316e-06 - accuracy: 0.9350\n",
      "Epoch 284/500\n",
      "7/7 [==============================] - 1s 141ms/step - loss: 4.6768e-06 - accuracy: 0.9050\n",
      "Epoch 285/500\n",
      "7/7 [==============================] - 1s 131ms/step - loss: 2.8921e-06 - accuracy: 0.9350\n",
      "Epoch 286/500\n",
      "7/7 [==============================] - 1s 137ms/step - loss: 3.6426e-06 - accuracy: 0.9300\n",
      "Epoch 287/500\n",
      "7/7 [==============================] - 1s 144ms/step - loss: 2.9909e-06 - accuracy: 0.9300\n",
      "Epoch 288/500\n",
      "7/7 [==============================] - 1s 134ms/step - loss: 3.8191e-06 - accuracy: 0.9300\n",
      "Epoch 289/500\n",
      "7/7 [==============================] - 1s 134ms/step - loss: 3.6805e-06 - accuracy: 0.9150\n",
      "Epoch 290/500\n",
      "7/7 [==============================] - 1s 140ms/step - loss: 3.6109e-06 - accuracy: 0.9100\n",
      "Epoch 291/500\n",
      "7/7 [==============================] - 1s 126ms/step - loss: 3.2188e-06 - accuracy: 0.9400\n",
      "Epoch 292/500\n",
      "7/7 [==============================] - 1s 129ms/step - loss: 3.9811e-06 - accuracy: 0.9050\n",
      "Epoch 293/500\n",
      "7/7 [==============================] - 1s 141ms/step - loss: 3.7159e-06 - accuracy: 0.8900\n",
      "Epoch 294/500\n",
      "7/7 [==============================] - 1s 131ms/step - loss: 3.3127e-06 - accuracy: 0.9050\n",
      "Epoch 295/500\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 3.2435e-06 - accuracy: 0.9500\n",
      "Epoch 296/500\n",
      "7/7 [==============================] - 1s 140ms/step - loss: 3.2566e-06 - accuracy: 0.9350\n",
      "Epoch 297/500\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 3.3701e-06 - accuracy: 0.9500\n",
      "Epoch 298/500\n",
      "7/7 [==============================] - 1s 129ms/step - loss: 2.9130e-06 - accuracy: 0.9200\n",
      "Epoch 299/500\n",
      "7/7 [==============================] - 1s 135ms/step - loss: 3.4800e-06 - accuracy: 0.9450\n",
      "Epoch 300/500\n",
      "7/7 [==============================] - 1s 131ms/step - loss: 3.4382e-06 - accuracy: 0.9150\n",
      "Epoch 301/500\n",
      "7/7 [==============================] - 1s 131ms/step - loss: 3.0620e-06 - accuracy: 0.9300\n",
      "Epoch 302/500\n",
      "7/7 [==============================] - 1s 137ms/step - loss: 3.1530e-06 - accuracy: 0.9600\n",
      "Epoch 303/500\n",
      "7/7 [==============================] - 1s 129ms/step - loss: 3.2870e-06 - accuracy: 0.9350\n",
      "Epoch 304/500\n",
      "7/7 [==============================] - 1s 129ms/step - loss: 3.8443e-06 - accuracy: 0.9000\n",
      "Epoch 305/500\n",
      "7/7 [==============================] - 1s 140ms/step - loss: 3.6788e-06 - accuracy: 0.9100\n",
      "Epoch 306/500\n",
      "7/7 [==============================] - 1s 132ms/step - loss: 4.9427e-06 - accuracy: 0.9050\n",
      "Epoch 307/500\n",
      "7/7 [==============================] - 1s 131ms/step - loss: 3.3391e-06 - accuracy: 0.9150\n",
      "Epoch 308/500\n",
      "7/7 [==============================] - 1s 141ms/step - loss: 2.7954e-06 - accuracy: 0.9450\n",
      "Epoch 309/500\n",
      "7/7 [==============================] - 1s 138ms/step - loss: 3.0986e-06 - accuracy: 0.9300\n",
      "Epoch 310/500\n",
      "7/7 [==============================] - 1s 126ms/step - loss: 2.9285e-06 - accuracy: 0.9200\n",
      "Epoch 311/500\n",
      "7/7 [==============================] - 1s 134ms/step - loss: 3.9865e-06 - accuracy: 0.9500\n",
      "Epoch 312/500\n",
      "7/7 [==============================] - 1s 138ms/step - loss: 3.5780e-06 - accuracy: 0.9200\n",
      "Epoch 313/500\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 3.4264e-06 - accuracy: 0.9100\n",
      "Epoch 314/500\n",
      "7/7 [==============================] - 1s 152ms/step - loss: 3.3003e-06 - accuracy: 0.9350\n",
      "Epoch 315/500\n",
      "7/7 [==============================] - 1s 130ms/step - loss: 3.3432e-06 - accuracy: 0.9050\n",
      "Epoch 316/500\n",
      "7/7 [==============================] - 1s 130ms/step - loss: 2.7306e-06 - accuracy: 0.9250\n",
      "Epoch 317/500\n",
      "7/7 [==============================] - 1s 140ms/step - loss: 3.1319e-06 - accuracy: 0.9450\n",
      "Epoch 318/500\n",
      "7/7 [==============================] - 1s 135ms/step - loss: 2.9974e-06 - accuracy: 0.9000\n",
      "Epoch 319/500\n",
      "7/7 [==============================] - 1s 126ms/step - loss: 3.8908e-06 - accuracy: 0.8950\n",
      "Epoch 320/500\n",
      "7/7 [==============================] - 1s 150ms/step - loss: 3.0431e-06 - accuracy: 0.9400\n",
      "Epoch 321/500\n",
      "7/7 [==============================] - 1s 133ms/step - loss: 3.1163e-06 - accuracy: 0.9450\n",
      "Epoch 322/500\n",
      "7/7 [==============================] - 1s 123ms/step - loss: 2.8778e-06 - accuracy: 0.9650\n",
      "Epoch 323/500\n",
      "7/7 [==============================] - 1s 133ms/step - loss: 2.6159e-06 - accuracy: 0.9500\n",
      "Epoch 324/500\n",
      "7/7 [==============================] - 1s 136ms/step - loss: 3.3098e-06 - accuracy: 0.9250\n",
      "Epoch 325/500\n",
      "7/7 [==============================] - 1s 125ms/step - loss: 3.8522e-06 - accuracy: 0.9150\n",
      "Epoch 326/500\n",
      "7/7 [==============================] - 1s 134ms/step - loss: 3.4942e-06 - accuracy: 0.8900\n",
      "Epoch 327/500\n",
      "7/7 [==============================] - 1s 146ms/step - loss: 2.8517e-06 - accuracy: 0.9450\n",
      "Epoch 328/500\n",
      "7/7 [==============================] - 1s 125ms/step - loss: 2.9691e-06 - accuracy: 0.9200\n",
      "Epoch 329/500\n",
      "7/7 [==============================] - 1s 136ms/step - loss: 2.5744e-06 - accuracy: 0.9100\n",
      "Epoch 330/500\n",
      "7/7 [==============================] - 1s 143ms/step - loss: 2.6510e-06 - accuracy: 0.9400\n",
      "Epoch 331/500\n",
      "7/7 [==============================] - 1s 124ms/step - loss: 2.8815e-06 - accuracy: 0.9050\n",
      "Epoch 332/500\n",
      "7/7 [==============================] - 1s 130ms/step - loss: 2.9952e-06 - accuracy: 0.9300\n",
      "Epoch 333/500\n",
      "7/7 [==============================] - 1s 145ms/step - loss: 3.1332e-06 - accuracy: 0.9200\n",
      "Epoch 334/500\n",
      "7/7 [==============================] - 1s 130ms/step - loss: 3.9389e-06 - accuracy: 0.9100\n",
      "Epoch 335/500\n",
      "7/7 [==============================] - 1s 129ms/step - loss: 3.3575e-06 - accuracy: 0.9000\n",
      "Epoch 336/500\n",
      "7/7 [==============================] - 1s 161ms/step - loss: 3.2860e-06 - accuracy: 0.9150\n",
      "Epoch 337/500\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 3.0354e-06 - accuracy: 0.9300\n",
      "Epoch 338/500\n",
      "7/7 [==============================] - 1s 130ms/step - loss: 3.3622e-06 - accuracy: 0.9100\n",
      "Epoch 339/500\n",
      "7/7 [==============================] - 1s 145ms/step - loss: 3.5587e-06 - accuracy: 0.9300\n",
      "Epoch 340/500\n",
      "7/7 [==============================] - 1s 129ms/step - loss: 3.7645e-06 - accuracy: 0.9050\n",
      "Epoch 341/500\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 3.3749e-06 - accuracy: 0.9250\n",
      "Epoch 342/500\n",
      "7/7 [==============================] - 1s 158ms/step - loss: 3.1294e-06 - accuracy: 0.9300\n",
      "Epoch 343/500\n",
      "7/7 [==============================] - 1s 143ms/step - loss: 2.5427e-06 - accuracy: 0.9450\n",
      "Epoch 344/500\n",
      "7/7 [==============================] - 1s 131ms/step - loss: 2.6019e-06 - accuracy: 0.9300\n",
      "Epoch 345/500\n",
      "7/7 [==============================] - 1s 155ms/step - loss: 2.5568e-06 - accuracy: 0.9250\n",
      "Epoch 346/500\n",
      "7/7 [==============================] - 1s 138ms/step - loss: 2.2579e-06 - accuracy: 0.9200\n",
      "Epoch 347/500\n",
      "7/7 [==============================] - 1s 130ms/step - loss: 2.7566e-06 - accuracy: 0.9050\n",
      "Epoch 348/500\n",
      "7/7 [==============================] - 1s 147ms/step - loss: 3.1860e-06 - accuracy: 0.9250\n",
      "Epoch 349/500\n",
      "7/7 [==============================] - 1s 137ms/step - loss: 3.2059e-06 - accuracy: 0.9550\n",
      "Epoch 350/500\n",
      "7/7 [==============================] - 1s 129ms/step - loss: 3.3545e-06 - accuracy: 0.9250\n",
      "Epoch 351/500\n",
      "7/7 [==============================] - 1s 141ms/step - loss: 2.8335e-06 - accuracy: 0.9250\n",
      "Epoch 352/500\n",
      "7/7 [==============================] - 1s 140ms/step - loss: 3.4609e-06 - accuracy: 0.9300\n",
      "Epoch 353/500\n",
      "7/7 [==============================] - 1s 123ms/step - loss: 4.2561e-06 - accuracy: 0.9400\n",
      "Epoch 354/500\n",
      "7/7 [==============================] - 1s 143ms/step - loss: 3.6621e-06 - accuracy: 0.9250\n",
      "Epoch 355/500\n",
      "7/7 [==============================] - 1s 142ms/step - loss: 3.4360e-06 - accuracy: 0.9050\n",
      "Epoch 356/500\n",
      "7/7 [==============================] - 1s 123ms/step - loss: 2.5113e-06 - accuracy: 0.9350\n",
      "Epoch 357/500\n",
      "7/7 [==============================] - 1s 145ms/step - loss: 2.8246e-06 - accuracy: 0.9550\n",
      "Epoch 358/500\n",
      "7/7 [==============================] - 1s 146ms/step - loss: 3.7451e-06 - accuracy: 0.9250\n",
      "Epoch 359/500\n",
      "7/7 [==============================] - 1s 119ms/step - loss: 2.8392e-06 - accuracy: 0.9150\n",
      "Epoch 360/500\n",
      "7/7 [==============================] - 1s 144ms/step - loss: 2.9219e-06 - accuracy: 0.9500\n",
      "Epoch 361/500\n",
      "7/7 [==============================] - 1s 139ms/step - loss: 2.5694e-06 - accuracy: 0.9450\n",
      "Epoch 362/500\n",
      "7/7 [==============================] - 1s 113ms/step - loss: 2.8983e-06 - accuracy: 0.9250\n",
      "Epoch 363/500\n",
      "7/7 [==============================] - 1s 145ms/step - loss: 3.2998e-06 - accuracy: 0.9000\n",
      "Epoch 364/500\n",
      "7/7 [==============================] - 1s 159ms/step - loss: 2.9958e-06 - accuracy: 0.8900\n",
      "Epoch 365/500\n",
      "7/7 [==============================] - 1s 117ms/step - loss: 2.6892e-06 - accuracy: 0.9350\n",
      "Epoch 366/500\n",
      "7/7 [==============================] - 1s 130ms/step - loss: 2.6380e-06 - accuracy: 0.9350\n",
      "Epoch 367/500\n",
      "7/7 [==============================] - 1s 149ms/step - loss: 2.5118e-06 - accuracy: 0.9450\n",
      "Epoch 368/500\n",
      "7/7 [==============================] - 1s 123ms/step - loss: 2.2217e-06 - accuracy: 0.9850\n",
      "Epoch 369/500\n",
      "7/7 [==============================] - 1s 121ms/step - loss: 2.7683e-06 - accuracy: 0.9200\n",
      "Epoch 370/500\n",
      "7/7 [==============================] - 1s 143ms/step - loss: 3.1485e-06 - accuracy: 0.9000\n",
      "Epoch 371/500\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 2.9666e-06 - accuracy: 0.9150\n",
      "Epoch 372/500\n",
      "7/7 [==============================] - 1s 117ms/step - loss: 2.5097e-06 - accuracy: 0.9300\n",
      "Epoch 373/500\n",
      "7/7 [==============================] - 1s 141ms/step - loss: 2.3008e-06 - accuracy: 0.9450\n",
      "Epoch 374/500\n",
      "7/7 [==============================] - 1s 141ms/step - loss: 2.5817e-06 - accuracy: 0.9550\n",
      "Epoch 375/500\n",
      "7/7 [==============================] - 1s 115ms/step - loss: 2.9223e-06 - accuracy: 0.9200\n",
      "Epoch 376/500\n",
      "7/7 [==============================] - 1s 140ms/step - loss: 2.9093e-06 - accuracy: 0.9150\n",
      "Epoch 377/500\n",
      "7/7 [==============================] - 1s 139ms/step - loss: 2.7833e-06 - accuracy: 0.9450\n",
      "Epoch 378/500\n",
      "7/7 [==============================] - 1s 114ms/step - loss: 2.4550e-06 - accuracy: 0.9300\n",
      "Epoch 379/500\n",
      "7/7 [==============================] - 1s 136ms/step - loss: 2.4781e-06 - accuracy: 0.9500\n",
      "Epoch 380/500\n",
      "7/7 [==============================] - 1s 147ms/step - loss: 2.7359e-06 - accuracy: 0.9400\n",
      "Epoch 381/500\n",
      "7/7 [==============================] - 1s 115ms/step - loss: 2.5931e-06 - accuracy: 0.9450\n",
      "Epoch 382/500\n",
      "7/7 [==============================] - 1s 139ms/step - loss: 2.5937e-06 - accuracy: 0.9100\n",
      "Epoch 383/500\n",
      "7/7 [==============================] - 1s 157ms/step - loss: 3.1972e-06 - accuracy: 0.9500\n",
      "Epoch 384/500\n",
      "7/7 [==============================] - 1s 120ms/step - loss: 2.6792e-06 - accuracy: 0.9450\n",
      "Epoch 385/500\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 2.3880e-06 - accuracy: 0.9250\n",
      "Epoch 386/500\n",
      "7/7 [==============================] - 1s 147ms/step - loss: 2.2360e-06 - accuracy: 0.9350\n",
      "Epoch 387/500\n",
      "7/7 [==============================] - 1s 121ms/step - loss: 2.0723e-06 - accuracy: 0.9750\n",
      "Epoch 388/500\n",
      "7/7 [==============================] - 1s 126ms/step - loss: 2.1899e-06 - accuracy: 0.9350\n",
      "Epoch 389/500\n",
      "7/7 [==============================] - 1s 152ms/step - loss: 1.9654e-06 - accuracy: 0.9250\n",
      "Epoch 390/500\n",
      "7/7 [==============================] - 1s 122ms/step - loss: 2.1181e-06 - accuracy: 0.9600\n",
      "Epoch 391/500\n",
      "7/7 [==============================] - 1s 117ms/step - loss: 2.1102e-06 - accuracy: 0.9150\n",
      "Epoch 392/500\n",
      "7/7 [==============================] - 1s 150ms/step - loss: 2.4104e-06 - accuracy: 0.9350\n",
      "Epoch 393/500\n",
      "7/7 [==============================] - 1s 125ms/step - loss: 2.9643e-06 - accuracy: 0.8850\n",
      "Epoch 394/500\n",
      "7/7 [==============================] - 1s 116ms/step - loss: 4.0190e-06 - accuracy: 0.8800\n",
      "Epoch 395/500\n",
      "7/7 [==============================] - 1s 138ms/step - loss: 4.2870e-06 - accuracy: 0.9250\n",
      "Epoch 396/500\n",
      "7/7 [==============================] - 1s 131ms/step - loss: 4.1287e-06 - accuracy: 0.9150\n",
      "Epoch 397/500\n",
      "7/7 [==============================] - 1s 115ms/step - loss: 4.5220e-06 - accuracy: 0.9150\n",
      "Epoch 398/500\n",
      "7/7 [==============================] - 1s 148ms/step - loss: 3.9794e-06 - accuracy: 0.9300\n",
      "Epoch 399/500\n",
      "7/7 [==============================] - 1s 145ms/step - loss: 4.3246e-06 - accuracy: 0.8650\n",
      "Epoch 400/500\n",
      "7/7 [==============================] - 1s 111ms/step - loss: 3.2792e-06 - accuracy: 0.9150\n",
      "Epoch 401/500\n",
      "7/7 [==============================] - 1s 141ms/step - loss: 4.0193e-06 - accuracy: 0.9350\n",
      "Epoch 402/500\n",
      "7/7 [==============================] - 1s 143ms/step - loss: 3.0992e-06 - accuracy: 0.8550\n",
      "Epoch 403/500\n",
      "7/7 [==============================] - 1s 118ms/step - loss: 2.7299e-06 - accuracy: 0.9250\n",
      "Epoch 404/500\n",
      "7/7 [==============================] - 1s 153ms/step - loss: 2.2207e-06 - accuracy: 0.9400\n",
      "Epoch 405/500\n",
      "7/7 [==============================] - 1s 141ms/step - loss: 2.2380e-06 - accuracy: 0.9750\n",
      "Epoch 406/500\n",
      "7/7 [==============================] - 1s 121ms/step - loss: 2.7173e-06 - accuracy: 0.9000\n",
      "Epoch 407/500\n",
      "7/7 [==============================] - 1s 146ms/step - loss: 2.7266e-06 - accuracy: 0.9300\n",
      "Epoch 408/500\n",
      "7/7 [==============================] - 1s 143ms/step - loss: 3.0301e-06 - accuracy: 0.8850\n",
      "Epoch 409/500\n",
      "7/7 [==============================] - 1s 133ms/step - loss: 2.5719e-06 - accuracy: 0.9150\n",
      "Epoch 410/500\n",
      "7/7 [==============================] - 1s 144ms/step - loss: 2.4142e-06 - accuracy: 0.9200\n",
      "Epoch 411/500\n",
      "7/7 [==============================] - 1s 135ms/step - loss: 3.0928e-06 - accuracy: 0.9100\n",
      "Epoch 412/500\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 2.3821e-06 - accuracy: 0.9350\n",
      "Epoch 413/500\n",
      "7/7 [==============================] - 1s 138ms/step - loss: 2.3838e-06 - accuracy: 0.9050\n",
      "Epoch 414/500\n",
      "7/7 [==============================] - 1s 147ms/step - loss: 2.7971e-06 - accuracy: 0.9350\n",
      "Epoch 415/500\n",
      "7/7 [==============================] - 1s 126ms/step - loss: 2.4317e-06 - accuracy: 0.9550\n",
      "Epoch 416/500\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 2.1195e-06 - accuracy: 0.9650\n",
      "Epoch 417/500\n",
      "7/7 [==============================] - 1s 143ms/step - loss: 1.9787e-06 - accuracy: 0.9200\n",
      "Epoch 418/500\n",
      "7/7 [==============================] - 1s 121ms/step - loss: 2.0523e-06 - accuracy: 0.9200\n",
      "Epoch 419/500\n",
      "7/7 [==============================] - 1s 131ms/step - loss: 2.1316e-06 - accuracy: 0.9450\n",
      "Epoch 420/500\n",
      "7/7 [==============================] - 1s 145ms/step - loss: 2.2410e-06 - accuracy: 0.9050\n",
      "Epoch 421/500\n",
      "7/7 [==============================] - 1s 122ms/step - loss: 1.9354e-06 - accuracy: 0.9450\n",
      "Epoch 422/500\n",
      "7/7 [==============================] - 1s 129ms/step - loss: 2.6808e-06 - accuracy: 0.9550\n",
      "Epoch 423/500\n",
      "7/7 [==============================] - 1s 148ms/step - loss: 2.4127e-06 - accuracy: 0.9050\n",
      "Epoch 424/500\n",
      "7/7 [==============================] - 1s 117ms/step - loss: 2.1777e-06 - accuracy: 0.9650\n",
      "Epoch 425/500\n",
      "7/7 [==============================] - 1s 122ms/step - loss: 2.1080e-06 - accuracy: 0.9500\n",
      "Epoch 426/500\n",
      "7/7 [==============================] - 1s 155ms/step - loss: 2.4269e-06 - accuracy: 0.9650\n",
      "Epoch 427/500\n",
      "7/7 [==============================] - 1s 117ms/step - loss: 2.2398e-06 - accuracy: 0.9450\n",
      "Epoch 428/500\n",
      "7/7 [==============================] - 1s 126ms/step - loss: 2.7354e-06 - accuracy: 0.9150\n",
      "Epoch 429/500\n",
      "7/7 [==============================] - 1s 154ms/step - loss: 2.2719e-06 - accuracy: 0.9550\n",
      "Epoch 430/500\n",
      "7/7 [==============================] - 1s 115ms/step - loss: 2.0625e-06 - accuracy: 0.9450\n",
      "Epoch 431/500\n",
      "7/7 [==============================] - 1s 126ms/step - loss: 1.8755e-06 - accuracy: 0.9300\n",
      "Epoch 432/500\n",
      "7/7 [==============================] - 1s 152ms/step - loss: 1.9159e-06 - accuracy: 0.9400\n",
      "Epoch 433/500\n",
      "7/7 [==============================] - 1s 124ms/step - loss: 2.2977e-06 - accuracy: 0.9100\n",
      "Epoch 434/500\n",
      "7/7 [==============================] - 1s 122ms/step - loss: 2.4085e-06 - accuracy: 0.8950\n",
      "Epoch 435/500\n",
      "7/7 [==============================] - 1s 152ms/step - loss: 2.6337e-06 - accuracy: 0.9500\n",
      "Epoch 436/500\n",
      "7/7 [==============================] - 1s 121ms/step - loss: 1.9021e-06 - accuracy: 0.8950\n",
      "Epoch 437/500\n",
      "7/7 [==============================] - 1s 122ms/step - loss: 2.1933e-06 - accuracy: 0.9450\n",
      "Epoch 438/500\n",
      "7/7 [==============================] - 1s 147ms/step - loss: 2.0632e-06 - accuracy: 0.9100\n",
      "Epoch 439/500\n",
      "7/7 [==============================] - 1s 125ms/step - loss: 2.1686e-06 - accuracy: 0.9300\n",
      "Epoch 440/500\n",
      "7/7 [==============================] - 1s 124ms/step - loss: 1.9117e-06 - accuracy: 0.9200\n",
      "Epoch 441/500\n",
      "7/7 [==============================] - 1s 144ms/step - loss: 1.8858e-06 - accuracy: 0.9150\n",
      "Epoch 442/500\n",
      "7/7 [==============================] - 1s 131ms/step - loss: 1.9032e-06 - accuracy: 0.9450\n",
      "Epoch 443/500\n",
      "7/7 [==============================] - 1s 121ms/step - loss: 2.3584e-06 - accuracy: 0.9100\n",
      "Epoch 444/500\n",
      "7/7 [==============================] - 1s 137ms/step - loss: 2.4307e-06 - accuracy: 0.9400\n",
      "Epoch 445/500\n",
      "7/7 [==============================] - 1s 137ms/step - loss: 3.5303e-06 - accuracy: 0.9250\n",
      "Epoch 446/500\n",
      "7/7 [==============================] - 1s 118ms/step - loss: 2.5016e-06 - accuracy: 0.9200\n",
      "Epoch 447/500\n",
      "7/7 [==============================] - 1s 139ms/step - loss: 1.9356e-06 - accuracy: 0.9500\n",
      "Epoch 448/500\n",
      "7/7 [==============================] - 1s 157ms/step - loss: 1.8002e-06 - accuracy: 0.9350\n",
      "Epoch 449/500\n",
      "7/7 [==============================] - 1s 123ms/step - loss: 2.2026e-06 - accuracy: 0.9500\n",
      "Epoch 450/500\n",
      "7/7 [==============================] - 1s 158ms/step - loss: 2.0499e-06 - accuracy: 0.9600\n",
      "Epoch 451/500\n",
      "7/7 [==============================] - 1s 131ms/step - loss: 2.0704e-06 - accuracy: 0.9350\n",
      "Epoch 452/500\n",
      "7/7 [==============================] - 1s 116ms/step - loss: 2.0854e-06 - accuracy: 0.9500\n",
      "Epoch 453/500\n",
      "7/7 [==============================] - 1s 153ms/step - loss: 2.0048e-06 - accuracy: 0.9450\n",
      "Epoch 454/500\n",
      "7/7 [==============================] - 1s 132ms/step - loss: 2.0714e-06 - accuracy: 0.9300\n",
      "Epoch 455/500\n",
      "7/7 [==============================] - 1s 122ms/step - loss: 1.8664e-06 - accuracy: 0.9000\n",
      "Epoch 456/500\n",
      "7/7 [==============================] - 1s 149ms/step - loss: 1.8431e-06 - accuracy: 0.9150\n",
      "Epoch 457/500\n",
      "7/7 [==============================] - 1s 131ms/step - loss: 2.3125e-06 - accuracy: 0.9150\n",
      "Epoch 458/500\n",
      "7/7 [==============================] - 1s 121ms/step - loss: 1.9474e-06 - accuracy: 0.9500\n",
      "Epoch 459/500\n",
      "7/7 [==============================] - 1s 144ms/step - loss: 2.3480e-06 - accuracy: 0.9100\n",
      "Epoch 460/500\n",
      "7/7 [==============================] - 1s 134ms/step - loss: 2.1837e-06 - accuracy: 0.9250\n",
      "Epoch 461/500\n",
      "7/7 [==============================] - 1s 119ms/step - loss: 2.0026e-06 - accuracy: 0.9000\n",
      "Epoch 462/500\n",
      "7/7 [==============================] - 1s 145ms/step - loss: 1.7077e-06 - accuracy: 0.9500\n",
      "Epoch 463/500\n",
      "7/7 [==============================] - 1s 130ms/step - loss: 2.1715e-06 - accuracy: 0.9050\n",
      "Epoch 464/500\n",
      "7/7 [==============================] - 1s 117ms/step - loss: 2.0844e-06 - accuracy: 0.9300\n",
      "Epoch 465/500\n",
      "7/7 [==============================] - 1s 144ms/step - loss: 2.2786e-06 - accuracy: 0.9200\n",
      "Epoch 466/500\n",
      "7/7 [==============================] - 1s 136ms/step - loss: 2.6874e-06 - accuracy: 0.8900\n",
      "Epoch 467/500\n",
      "7/7 [==============================] - 1s 114ms/step - loss: 2.1263e-06 - accuracy: 0.9250\n",
      "Epoch 468/500\n",
      "7/7 [==============================] - 1s 147ms/step - loss: 1.7778e-06 - accuracy: 0.9800\n",
      "Epoch 469/500\n",
      "7/7 [==============================] - 1s 138ms/step - loss: 2.0843e-06 - accuracy: 0.9450\n",
      "Epoch 470/500\n",
      "7/7 [==============================] - 1s 122ms/step - loss: 1.8478e-06 - accuracy: 0.9450\n",
      "Epoch 471/500\n",
      "7/7 [==============================] - 1s 153ms/step - loss: 1.8438e-06 - accuracy: 0.9200\n",
      "Epoch 472/500\n",
      "7/7 [==============================] - 1s 151ms/step - loss: 2.1879e-06 - accuracy: 0.9450\n",
      "Epoch 473/500\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 2.0690e-06 - accuracy: 0.9050\n",
      "Epoch 474/500\n",
      "7/7 [==============================] - 1s 153ms/step - loss: 2.3306e-06 - accuracy: 0.9300\n",
      "Epoch 475/500\n",
      "7/7 [==============================] - 1s 147ms/step - loss: 2.3066e-06 - accuracy: 0.9400\n",
      "Epoch 476/500\n",
      "7/7 [==============================] - 1s 123ms/step - loss: 1.9527e-06 - accuracy: 0.8900\n",
      "Epoch 477/500\n",
      "7/7 [==============================] - 1s 150ms/step - loss: 1.9027e-06 - accuracy: 0.9300\n",
      "Epoch 478/500\n",
      "7/7 [==============================] - 1s 138ms/step - loss: 1.9275e-06 - accuracy: 0.9250\n",
      "Epoch 479/500\n",
      "7/7 [==============================] - 1s 119ms/step - loss: 1.6112e-06 - accuracy: 0.9350\n",
      "Epoch 480/500\n",
      "7/7 [==============================] - 1s 135ms/step - loss: 1.8577e-06 - accuracy: 0.9450\n",
      "Epoch 481/500\n",
      "7/7 [==============================] - 1s 141ms/step - loss: 1.7452e-06 - accuracy: 0.9600\n",
      "Epoch 482/500\n",
      "7/7 [==============================] - 1s 124ms/step - loss: 1.6951e-06 - accuracy: 0.9350\n",
      "Epoch 483/500\n",
      "7/7 [==============================] - 1s 131ms/step - loss: 1.8047e-06 - accuracy: 0.9200\n",
      "Epoch 484/500\n",
      "7/7 [==============================] - 1s 140ms/step - loss: 1.6021e-06 - accuracy: 0.9550\n",
      "Epoch 485/500\n",
      "7/7 [==============================] - 1s 119ms/step - loss: 1.6552e-06 - accuracy: 0.9400\n",
      "Epoch 486/500\n",
      "7/7 [==============================] - 1s 131ms/step - loss: 1.6450e-06 - accuracy: 0.9500\n",
      "Epoch 487/500\n",
      "7/7 [==============================] - 1s 146ms/step - loss: 1.5026e-06 - accuracy: 0.9550\n",
      "Epoch 488/500\n",
      "7/7 [==============================] - 1s 119ms/step - loss: 1.7166e-06 - accuracy: 0.9500\n",
      "Epoch 489/500\n",
      "7/7 [==============================] - 1s 136ms/step - loss: 1.9318e-06 - accuracy: 0.9300\n",
      "Epoch 490/500\n",
      "7/7 [==============================] - 1s 150ms/step - loss: 2.5614e-06 - accuracy: 0.9300\n",
      "Epoch 491/500\n",
      "7/7 [==============================] - 1s 124ms/step - loss: 2.7660e-06 - accuracy: 0.8850\n",
      "Epoch 492/500\n",
      "7/7 [==============================] - 1s 135ms/step - loss: 2.2694e-06 - accuracy: 0.9250\n",
      "Epoch 493/500\n",
      "7/7 [==============================] - 1s 140ms/step - loss: 2.2562e-06 - accuracy: 0.9400\n",
      "Epoch 494/500\n",
      "7/7 [==============================] - 1s 115ms/step - loss: 2.0253e-06 - accuracy: 0.9300\n",
      "Epoch 495/500\n",
      "7/7 [==============================] - 1s 132ms/step - loss: 2.1059e-06 - accuracy: 0.9650\n",
      "Epoch 496/500\n",
      "7/7 [==============================] - 1s 139ms/step - loss: 2.1583e-06 - accuracy: 0.9200\n",
      "Epoch 497/500\n",
      "7/7 [==============================] - 1s 125ms/step - loss: 2.3577e-06 - accuracy: 0.9200\n",
      "Epoch 498/500\n",
      "7/7 [==============================] - 1s 125ms/step - loss: 2.5455e-06 - accuracy: 0.9150\n",
      "Epoch 499/500\n",
      "7/7 [==============================] - 1s 143ms/step - loss: 2.4862e-06 - accuracy: 0.9150\n",
      "Epoch 500/500\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 1.8836e-06 - accuracy: 0.9250\n"
     ]
    }
   ],
   "source": [
    "rediction=model.fit(x_train,y_train,epochs=500,batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x15896ec4250>]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(rediction.history[\"loss\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 1s 34ms/step - loss: 3.6038e-05 - accuracy: 0.5240\n",
      "Loss = 3.603815639507957e-05\n",
      "Test Accuracy = 0.5239999890327454\n"
     ]
    }
   ],
   "source": [
    "preds = model.evaluate(x_test, y_test)\n",
    "print (\"Loss = \" + str(preds[0]))\n",
    "print (\"Test Accuracy = \" + str(preds[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_data=x_train[10]\n",
    "Y_data=Y_data.reshape(1,100,100,-1)\n",
    "print(Y_data.shape)\n",
    "q=model.predict(Y_data)\n",
    "print(q)\n",
    "print(y_train[10])\n",
    "mse = tf.keras.losses.MeanAbsoluteError()\n",
    "error=mse(q, y_train[10]).numpy()\n",
    "print(error)\n",
    "# print(q-y_train[100])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3523a396672f5aa74d0ca7efbc5200bb5adbcd905681922dc84f6183b6dba551"
  },
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
